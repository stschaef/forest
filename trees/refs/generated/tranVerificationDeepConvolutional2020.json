[{"DOI": "10.48550/arXiv.2004.05511", "URL": "http://arxiv.org/abs/2004.05511", "abstract": "Convolutional Neural Networks (CNN) have redefined stateof-the-art in many real-world applications, such as facial recognition, image classification, human pose estimation, and semantic segmentation. Despite their success, CNNs are vulnerable to adversarial attacks, where slight changes to their inputs may lead to sharp changes in their output in even well-trained networks. Set-based analysis methods can detect or prove the absence of bounded adversarial attacks, which can then be used to evaluate the effectiveness of neural network training methodology. Unfortunately, existing verification approaches have limited scalability in terms of the size of networks that can be analyzed.", "accessed": {"date-parts": [[2025, 2, 19]]}, "author": [{"family": "Tran", "given": "Hoang-Dung"}, {"family": "Bak", "given": "Stanley"}, {"family": "Xiang", "given": "Weiming"}, {"family": "Johnson", "given": "Taylor T."}], "id": "tranVerificationDeepConvolutional2020", "issued": {"date-parts": [[2020, 5, 14]]}, "keyword": "Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning", "language": "en-US", "status": "pre-published", "title": "Verification of Deep Convolutional Neural Networks Using ImageStars", "type": "article-journal", "original_bibtex": "@article{tranVerificationDeepConvolutional2020,\n title = {Verification of {{Deep Convolutional Neural Networks Using ImageStars}}},\n author = {Tran, Hoang-Dung and Bak, Stanley and Xiang, Weiming and Johnson, Taylor T.},\n date = {2020-05-14},\n doi = {10.48550/arXiv.2004.05511},\n url = {http://arxiv.org/abs/2004.05511},\n urldate = {2025-02-19},\n file = {/Users/stevenschaefer/Zotero/storage/V4T73R6M/Tran et al. - 2020 - Verification of Deep Convolutional Neural Networks.pdf},\n keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},\n pubstate = {prepublished},\n langid = {english},\n abstract = {Convolutional Neural Networks (CNN) have redefined stateof-the-art in many real-world applications, such as facial recognition, image classification, human pose estimation, and semantic segmentation. Despite their success, CNNs are vulnerable to adversarial attacks, where slight changes to their inputs may lead to sharp changes in their output in even well-trained networks. Set-based analysis methods can detect or prove the absence of bounded adversarial attacks, which can then be used to evaluate the effectiveness of neural network training methodology. Unfortunately, existing verification approaches have limited scalability in terms of the size of networks that can be analyzed.},\n eprintclass = {cs},\n eprinttype = {arXiv},\n eprint = {2004.05511}\n}\n"}]