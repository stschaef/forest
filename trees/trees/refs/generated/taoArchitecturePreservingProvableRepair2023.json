[{"DOI": "10.1145/3591238", "ISSN": "2475-1421", "URL": "https://dl.acm.org/doi/10.1145/3591238", "abstract": "Deep neural networks (DNNs) are becoming increasingly important components of software, and are considered the state-of-the-art solution for a number of problems, such as image recognition. However, DNNs are far from infallible, and incorrect behavior of DNNs can have disastrous real-world consequences. This paper addresses the problem of architecture-preserving V-polytope provable repair of DNNs. A V-polytope defines a convex bounded polytope using its vertex representation. V-polytope provable repair guarantees that the repaired DNN satisfies the given specification on the infinite set of points in the given V-polytope. An architecture-preserving repair only modifies the parameters of the DNN, without modifying its architecture. The repair has the flexibility to modify multiple layers of the DNN, and runs in polynomial time. It supports DNNs with activation functions that have some linear pieces, as well as fully-connected, convolutional, pooling and residual layers. To the best our knowledge, this is the first provable repair approach that has all of these features. We implement our approach in a tool called APRNN. Using MNIST, ImageNet, and ACAS Xu DNNs, we show that it has better efficiency, scalability, and generalization compared to PRDNN and REASSURE, prior provable repair methods that are not architecture preserving. CCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; \u2022 Theory of computation \u2192 Linear programming; \u2022 Software and its engineering \u2192 Software post-development issues.", "accessed": {"date-parts": [[2023, 11, 27]]}, "author": [{"family": "Tao", "given": "Zhe"}, {"family": "Nawas", "given": "Stephanie"}, {"family": "Mitchell", "given": "Jacqueline"}, {"family": "Thakur", "given": "Aditya V."}], "container-title": "Proceedings of the ACM on Programming Languages", "container-title-short": "Proc. ACM Program. Lang.", "id": "taoArchitecturePreservingProvableRepair2023", "issued": {"date-parts": [[2023, 6, 6]]}, "language": "en-US", "page": "443-467", "title": "Architecture-Preserving Provable Repair of Deep Neural Networks", "type": "article-journal", "volume": "7", "original_bibtex": "@article{taoArchitecturePreservingProvableRepair2023,\n title = {Architecture-{{Preserving Provable Repair}} of {{Deep Neural Networks}}},\n author = {Tao, Zhe and Nawas, Stephanie and Mitchell, Jacqueline and Thakur, Aditya V.},\n date = {2023-06-06},\n doi = {10.1145/3591238},\n url = {https://dl.acm.org/doi/10.1145/3591238},\n urldate = {2023-11-27},\n journaltitle = {Proceedings of the ACM on Programming Languages},\n volume = {7},\n pages = {443--467},\n file = {/Users/stevenschaefer/Zotero/storage/PZJLBU9Z/Tao et al. - 2023 - Architecture-Preserving Provable Repair of Deep Ne.pdf},\n langid = {english},\n issue = {PLDI},\n abstract = {Deep neural networks (DNNs) are becoming increasingly important components of software, and are considered the state-of-the-art solution for a number of problems, such as image recognition. However, DNNs are far from infallible, and incorrect behavior of DNNs can have disastrous real-world consequences. This paper addresses the problem of architecture-preserving V-polytope provable repair of DNNs. A V-polytope defines a convex bounded polytope using its vertex representation. V-polytope provable repair guarantees that the repaired DNN satisfies the given specification on the infinite set of points in the given V-polytope. An architecture-preserving repair only modifies the parameters of the DNN, without modifying its architecture. The repair has the flexibility to modify multiple layers of the DNN, and runs in polynomial time. It supports DNNs with activation functions that have some linear pieces, as well as fully-connected, convolutional, pooling and residual layers. To the best our knowledge, this is the first provable repair approach that has all of these features. We implement our approach in a tool called APRNN. Using MNIST, ImageNet, and ACAS Xu DNNs, we show that it has better efficiency, scalability, and generalization compared to PRDNN and REASSURE, prior provable repair methods that are not architecture preserving. CCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; \u2022 Theory of computation \u2192 Linear programming; \u2022 Software and its engineering \u2192 Software post-development issues.},\n issn = {2475-1421},\n shortjournal = {Proc. ACM Program. Lang.}\n}\n"}]