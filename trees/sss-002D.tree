\title{A Method for Verify Translational Invariance of Image Processing Neural Networks}
\date{2025-02-19T14:40:25Z}
\author{stevenschaefer}

\tag{halfbaked}

\import{base-macros}

\def\exp[x]{#{e\Sup{\x}}}
\def\N{#{N}}
\def\P{#{P}}
\def\D{#{D}}
\def\A{#{A}}
\def\I{#{I}}
\def\S{#{S}}
\def\St{#{S\Sup{T}}}
\def\log[x]{#{\mathsf{log(\x)}}}

\p{
My understanding for how one may prove a safety property \P for a neural network \N is as follows. First, express \P as an input-output property. That is, choose some region \D in the domain of \N to represent the inputs of interest. Further choose some region \A in the codomain of \N to describe safe outputs. Then describe \P as the property:
##{\forall x \in \D . \N(x) \in \A}
}

\p{
For instance, this is more or less how Reluplex, Marabou, and #{\alpha \beta}-CROWN each work. Squinting my eyes, this is the only such method for verifying a safety property for \N. That is, this is the only way to get a 100% guarantee that \P holds rather than some high measure of confidence.
}

\p{
I believe in this formalism I have an idea for how to express the property "\N is translationally invariant" where \N is an image classifying net.
}

\p{
To this end, we need a continuous artifact that captures what it means to translate an image. We can express an image as a matrix of pixels \I (or perhaps several parallel matrices if we care about color channels, but stick to a single grayscale matrix for now). To shift \I over by a single pixel, we may left-multiply \I by the shift matrix \S, where \S is the matrix filled with zeros and has 1's on the subdiagonal. Note that all the directions of shifting \I are captured by the combinations of left/right multiplication by \S,\St.
}

\p{
Shifting by multiple pixels is now expressed by the matrix #{\S\Sup{n} \I}, however this is still a discrete dynamical system. We don't have a continuous object by which we can test our safety property. My initial thought to continuousify this system was to express some sort of exponential flow by \S. That is, consider the matrix
##{\define{\S\Sup{t} \I}{\exp{t \log{S}}}}
}

\p{
#{\S\Sup{t} \I} then captures what it means to translate the image \I over by #{t}, a real-valued "amount of shifting". This choice of continuous artifact could then be used for a verification effort, however there are some issues related to numerical stability because \S isn't invertible. Because it is not invertible, #{\log{\S}} doesn't actually exist. So to make the above construction work, we need to mildly perturb \S and take a pseudoinverse. This sort of works and makes it so #{\S\Sup{t} \I} does capture some real valued shift, but it is only accurate when #{t} is small. So this is maybe problematic for our verification effort. This may be resolvable by chopping up the problem into subproblems, each of which is thin enough for the current iteration is accurate enough on that subproblem. However, there are two big things to consider.
}

\ol{
\li{The exponential approach above is probably too complicated. It seems likely that you may be able to take some (sequence of) linear interpolation(s) between the #{\S\Sup{n} \I}'s. This will still be some continuous object that captures a real-valued shift and it will be much more stable the approach above.}
\li{Even if we sort out which continuous object represents translation of an image, I cannot for the life of me train any neural network that is translationally invariant. So the proof method is useless if there is nothing that it would ever apply to.}
}

\p{
Precisely in this last point, I have mostly focused on trying to train a small CNN for MNIST handwritten digit classification that preserves the output class for small, reasonable translations of the digit. I've used data augmentation to predispose the network to being translationally invariant, and even though I can get a high degree of invariance, I cannot get a network that is invariant for all of the examples even in the training set or a reserved testing set.
}

\p{
It may be the case that the method I propose for measuring this invariance could be used to adversarially train a network to have better invariance. It may also be the case that all CNNs are bound to suffer from small degrees of translational sensitivity. I cannot find the citation at the moment, but there was a paper that suggested that CNNs suffer from weird issues of translational sensitivity that relate to the size of the convolutional window. So maybe this approach is doomed to fail anyway.
}

\p{
On the whole, I will say that machine learning verification almost sounds like an oxymoron. That is, if you have the expressivity to properly state a sophisticated safety property, then you likely understand the problem enough to not need to resort to machine learning in the first place. So almost tautologically, it seems that there cannot be satisfying verification of neural nets, as the tasks of machine learning and verification live on very different epistemic foundations.
}

\p{
The related works I could liberate from Zotero may be found below.
}
\transclude{sss-002F}
